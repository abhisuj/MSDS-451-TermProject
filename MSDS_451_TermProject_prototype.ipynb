{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c8230784",
        "outputId": "24dd04d2-c618-468b-eb77-18c5764b5d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (6.17.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (5.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel) (4.5.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.7)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Requirement already satisfied: pymoo in /usr/local/lib/python3.12/dist-packages (0.6.1.5)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from pymoo) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.12/dist-packages (from pymoo) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from pymoo) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.12/dist-packages (from pymoo) (1.8.0)\n",
            "Requirement already satisfied: cma>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from pymoo) (4.4.0)\n",
            "Requirement already satisfied: alive-progress in /usr/local/lib/python3.12/dist-packages (from pymoo) (3.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from pymoo) (0.3.8)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pymoo) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo) (2.9.0.post0)\n",
            "Requirement already satisfied: about-time==4.2.1 in /usr/local/lib/python3.12/dist-packages (from alive-progress->pymoo) (4.2.1)\n",
            "Requirement already satisfied: graphemeu==0.7.2 in /usr/local/lib/python3.12/dist-packages (from alive-progress->pymoo) (0.7.2)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->pymoo) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement talib-binary (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for talib-binary\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_drv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-792200276.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install talib-binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install TA-Lib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             self._str = self._format_parsed_parts(self.drive, self.root,\n\u001b[0m\u001b[1;32m    444\u001b[0m                                                   self._tail) or '.'\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_parse_path\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maltsep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maltsep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mdrv_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/posixpath.py\u001b[0m in \u001b[0;36msplitroot\u001b[0;34m(p)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%pip install ipykernel\n",
        "%pip install yfinance\n",
        "%pip install pymoo\n",
        "%pip install pandas numpy matplotlib seaborn\n",
        "%pip install scikit-learn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install talib-binary\n",
        "%pip install TA-Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iykTX3bRb_N7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import talib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.visualization.scatter import Scatter\n",
        "\n",
        "# --- Setup ---\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02fd1eba"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Define the tickers for the selected assets, including SPY\n",
        "# NOTE: SPY is included in the download for BENCHMARK comparison only\n",
        "#       It will be EXCLUDED from portfolio optimization and Bi-LSTM training\n",
        "#       SPY is only used to compare performance against the optimized portfolio\n",
        "\n",
        "#tickers = ['QQQ', 'SCHD', 'GLD', 'USDU', 'IEF', 'VNQ', 'SPY'] # Replaced BTC-USD with FXE (Invesco CurrencyShares Euro Trust)\n",
        "#tickers = ['QQQ', 'XLE', 'XLP', 'VEA', 'GLD', 'FXE', 'SCHH', 'SCHD', 'IEF', 'SPY']\n",
        "# Porfolio selection based on applying diverisification principles\n",
        "tickers = ['AAPL', 'GOOG', 'MSFT', 'NVDA',\n",
        "           'AMZN', 'AMD', 'INTC', 'META',\n",
        "           'COST', 'PG', 'KO', 'PEP', 'WMT',\n",
        "           'CME', 'AVGO', 'PFE', 'ABBV', 'HD',\n",
        "           'XOM', 'VDE',\n",
        "           'SCHD', 'VYM',\n",
        "           'VWO', 'VEA',\n",
        "           'GLD', 'SLV',\n",
        "           'FXY', 'FDIVX',\n",
        "           'TLT', 'SPLB',\n",
        "           'SPY']  # <-- SPY is BENCHMARK ONLY (excluded from portfolio)\n",
        "# Download historical data for the last 25 years\n",
        "data = yf.download(tickers, period='25y') # Changed period to 10y\n",
        "\n",
        "# We'll focus on the 'Close' price since 'Adj Close' is not directly available\n",
        "# Use dropna(how='all') to only remove rows where ALL tickers are NaN\n",
        "# This preserves SPY and other tickers even if some have missing data\n",
        "close_data = data['Close'].dropna(how='all')\n",
        "\n",
        "# Forward fill any remaining NaN values to handle missing data points\n",
        "close_data = close_data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# Ensure close_data index is timezone-naive for consistent alignment\n",
        "if close_data.index.tz is not None:\n",
        "    close_data.index = close_data.index.tz_localize(None)\n",
        "\n",
        "display(close_data.tail())\n",
        "print(f\"\\n Close data shape: {close_data.shape}\")\n",
        "print(f\" Tickers included: {list(close_data.columns)}\")\n",
        "print(f\" SPY included: {'SPY' in close_data.columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c89e9e9"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "daily_returns = close_data.pct_change().dropna()\n",
        "\n",
        "# Calculate cumulative returns\n",
        "cumulative_returns = (1 + daily_returns).cumprod() - 1\n",
        "\n",
        "display(cumulative_returns.tail())\n",
        "print(f\"\\n Cumulative returns shape: {cumulative_returns.shape}\")\n",
        "print(f\" SPY in cumulative_returns: {'SPY' in cumulative_returns.columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1145bcbc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot cumulative returns\n",
        "plt.figure(figsize=(12, 8))\n",
        "for ticker in cumulative_returns.columns:\n",
        "    plt.plot(cumulative_returns.index, cumulative_returns[ticker], label=ticker)\n",
        "\n",
        "plt.title('Cumulative Returns of Selected Assets and SPY (Last 10 Years)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Return')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25de7ab8"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Exclude SPY from the list of portfolio tickers\n",
        "portfolio_tickers = list(filter(lambda item: item != 'SPY', tickers))\n",
        "\n",
        "# Initialize dividend_data with the full date range of close_data and all portfolio tickers\n",
        "# Fill with 0.0 initially, as dividends are sparse.\n",
        "dividend_data = pd.DataFrame(0.0, index=close_data.index, columns=portfolio_tickers)\n",
        "\n",
        "for ticker in portfolio_tickers:\n",
        "    try:\n",
        "        dividends_series = yf.Ticker(ticker).dividends\n",
        "        if not dividends_series.empty:\n",
        "            # Ensure dividend index is timezone-naive for consistent alignment\n",
        "            if dividends_series.index.tz is not None:\n",
        "                dividends_series.index = dividends_series.index.tz_localize(None)\n",
        "\n",
        "            # Reindex the dividend series to the full close_data index and fill NaNs with 0\n",
        "            # Then assign it to the corresponding column in the pre-initialized dividend_data DataFrame\n",
        "            dividend_data[ticker] = dividends_series.reindex(close_data.index, fill_value=0.0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve dividend data for {ticker}: {e}\")\n",
        "\n",
        "# The dividend_data DataFrame is already filled with 0.0 for non-dividend days during reindexing,\n",
        "# so a final fillna(0) might be redundant but doesn't hurt.\n",
        "dividend_data = dividend_data.fillna(0)\n",
        "display(dividend_data.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35b2dad"
      },
      "outputs": [],
      "source": [
        "# To calculate the total dividends paid by an equal-weighted portfolio,\n",
        "# we can sum the dividends paid by each asset on each date and divide by the number of assets.\n",
        "# However, dividend payments are not daily. We'll resample to a more appropriate frequency, like monthly or quarterly,\n",
        "# and calculate the sum of dividends paid within each period. Let's use a monthly frequency.\n",
        "monthly_dividends = dividend_data.resample('M').sum()\n",
        "\n",
        "annualized_dividends = monthly_dividends.resample('Y').sum()\n",
        "display(\"Annualized Dividends Paid by Equal Weighted Portfolio (Last 10 Years):\")\n",
        "display(annualized_dividends)\n",
        "\n",
        "annualized_yield = annualized_dividends / close_data[annualized_dividends.columns]\n",
        "\n",
        "# Assuming an equal-weighted portfolio, the total dividend received by the portfolio in each month\n",
        "# is the sum of dividends from each asset divided by the number of assets.\n",
        "# We need to align this with the dates in our price data.\n",
        "# For simplicity here, we'll calculate the cumulative sum of dividends based on the dividend payment dates.\n",
        "# A more precise calculation would involve tracking the portfolio holdings over time.\n",
        "\n",
        "# Let's calculate the cumulative sum of dividends over time\n",
        "cumulative_dividends = monthly_dividends.sum(axis=1).cumsum()\n",
        "\n",
        "display(\"Cumulative Dividends Paid by Equal Weighted Portfolio (Monthly):\")\n",
        "display(cumulative_dividends.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a297b5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure dividend_data and close_data are available\n",
        "if 'dividend_data' not in globals() or 'close_data' not in globals():\n",
        "    raise ValueError(\"dividend_data or close_data not found. Please ensure these are loaded.\")\n",
        "\n",
        "# Calculate annual dividends paid per ticker\n",
        "# We'll reindex dividend_data to close_data's index and then resample annually\n",
        "aligned_daily_dividends = dividend_data.reindex(close_data.index).fillna(0)\n",
        "annual_dividends_sum = aligned_daily_dividends.resample('YE').sum()\n",
        "\n",
        "# Get year-end closing prices per ticker\n",
        "# Find the last available close price for each year\n",
        "annual_closing_prices = close_data.resample('YE').last()\n",
        "\n",
        "# Calculate the annualized dividend yield for each year and ticker\n",
        "# Handle potential division by zero by replacing 0s with NaN before division\n",
        "annual_dividend_yield_per_ticker = annual_dividends_sum.div(annual_closing_prices.replace(0, pd.NA))\n",
        "\n",
        "display(\"Annualized Dividend Yield (Annual Dividends / Year-End Price) per Ticker:\")\n",
        "display(annual_dividend_yield_per_ticker.fillna(0).tail())\n",
        "\n",
        "# Optional: Display mean annual yield over the period for comparison\n",
        "#display(\"\\nAverage Annualized Dividend Yield (Mean over years) per Ticker:\")\n",
        "#display(annual_dividend_yield_per_ticker.mean().fillna(0).sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6aeb0b5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure required dataframes are available\n",
        "if 'annual_dividend_yield_per_ticker' not in globals():\n",
        "    raise ValueError(\"annual_dividend_yield_per_ticker is not defined. Please run the previous cell to calculate it.\")\n",
        "if 'annual_closing_prices' not in globals():\n",
        "    raise ValueError(\"annual_closing_prices is not defined. Please run the previous cell to calculate it.\")\n",
        "if 'close_data' not in globals():\n",
        "    raise ValueError(\"close_data is not defined. Please ensure historical price data is downloaded.\")\n",
        "\n",
        "# 1. Calculate the total annual dividends in dollar amounts\n",
        "# We need to ensure indices and columns align for multiplication\n",
        "# Reindex annual_closing_prices to match annual_dividend_yield_per_ticker index and columns\n",
        "aligned_annual_closing_prices = annual_closing_prices.reindex_like(annual_dividend_yield_per_ticker)\n",
        "annual_dividend_dollars_per_ticker = annual_dividend_yield_per_ticker * aligned_annual_closing_prices\n",
        "\n",
        "# Fill any NaNs that might result from missing data points in either source with 0\n",
        "annual_dividend_dollars_per_ticker = annual_dividend_dollars_per_ticker.fillna(0)\n",
        "\n",
        "# 2. Determine the number of trading days for each year\n",
        "# Group close_data by year and count the number of entries\n",
        "trading_days_per_year = close_data.groupby(close_data.index.year).size()\n",
        "\n",
        "# Create a DataFrame to store the new aligned daily dividend data\n",
        "aligned_daily_dividend_data_new = pd.DataFrame(0.0, index=close_data.index, columns=portfolio_tickers)\n",
        "\n",
        "# 3. Distribute annual dividends evenly across trading days in each year\n",
        "for year in annual_dividend_dollars_per_ticker.index.year.unique():\n",
        "    if year in trading_days_per_year.index:\n",
        "        num_days = trading_days_per_year[year]\n",
        "        if num_days > 0:\n",
        "            # Get the annual dividend dollars for the current year\n",
        "            annual_dividends_for_year = annual_dividend_dollars_per_ticker.loc[annual_dividend_dollars_per_ticker.index.year == year].iloc[0]\n",
        "\n",
        "            # Calculate the daily average dividend amount for this year\n",
        "            daily_avg_dividends_for_year = annual_dividends_for_year / num_days\n",
        "\n",
        "            # Assign this daily average to all trading days within that year\n",
        "            year_mask = (aligned_daily_dividend_data_new.index.year == year)\n",
        "            # Ensure alignment of columns and use .loc for assignment\n",
        "            aligned_daily_dividend_data_new.loc[year_mask, daily_avg_dividends_for_year.index] = daily_avg_dividends_for_year.values\n",
        "\n",
        "# Display the head and tail of the newly created DataFrame\n",
        "display(\"New Aligned Daily Dividend Data (derived from annual yield/year-end price distributed daily):\")\n",
        "display(aligned_daily_dividend_data_new.tail())\n",
        "\n",
        "# Update the global aligned_daily_dividend_data variable with the new calculation\n",
        "aligned_daily_dividend_data = aligned_daily_dividend_data_new\n",
        "\n",
        "# Update nsga_ii_data with the average annual dividend yield (re-calculating using this new data)\n",
        "# Recalculate average_dividend_yield from the newly created aligned_daily_dividend_data\n",
        "cols = list(portfolio_tickers)\n",
        "\n",
        "# Reindex aligned_daily_dividend_data to close_data's index if not already (should be the case)\n",
        "div_df_aligned = aligned_daily_dividend_data.reindex(close_data.index).fillna(0)\n",
        "\n",
        "# Calculate annual dividends sum\n",
        "annual_div_sum = div_df_aligned.resample('YE').sum()\n",
        "\n",
        "# Get year-end closing prices per ticker for average yield calculation\n",
        "annual_close_prices_for_yield = close_data.resample('YE').last()\n",
        "\n",
        "# Calculate average annual yield based on new data\n",
        "# Handle potential division by zero\n",
        "avg_annual_yield = (annual_div_sum.div(annual_close_prices_for_yield.replace(0, np.nan))).mean(axis=0).reindex(cols).fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83739ffb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot cumulative dividends paid by the portfolio\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(cumulative_dividends.index, cumulative_dividends)\n",
        "\n",
        "plt.title('Cumulative Dividends Paid by Equal Weighted Portfolio (Last 10 Years)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Dividends')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faa335e6"
      },
      "outputs": [],
      "source": [
        "# Exclude SPY from the portfolio\n",
        "portfolio_assets = close_data.drop(columns=['SPY'])\n",
        "\n",
        "# Calculate daily returns for the portfolio assets\n",
        "portfolio_daily_returns = portfolio_assets.pct_change().dropna()\n",
        "\n",
        "# For a simple equal-weighted portfolio, the portfolio daily return is the average of the asset daily returns\n",
        "portfolio_returns = portfolio_daily_returns.mean(axis=1)\n",
        "\n",
        "# Calculate cumulative returns of the portfolio\n",
        "portfolio_cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
        "\n",
        "display(\"Portfolio Cumulative Returns (Equal Weighted):\")\n",
        "display(portfolio_cumulative_returns.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7acc2d2f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate annualized volatility (assuming 252 trading days in a year)\n",
        "annualized_volatility = portfolio_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Calculate Sharpe Ratio (assuming risk-free rate is 0 for simplicity in this example)\n",
        "sharpe_ratio = portfolio_returns.mean() / portfolio_returns.std() * np.sqrt(252)\n",
        "\n",
        "display(f\"Portfolio Annualized Volatility (Equal Weighted): {annualized_volatility:.4f}\")\n",
        "display(f\"Portfolio Sharpe Ratio (Equal Weighted): {sharpe_ratio:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1cf8467"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot cumulative returns with log scale\n",
        "plt.figure(figsize=(12, 8))\n",
        "for ticker in cumulative_returns.columns:\n",
        "    plt.plot(cumulative_returns.index, cumulative_returns[ticker] + 1, label=ticker)  # +1 to avoid log(0)\n",
        "\n",
        "plt.yscale('log')  # Enable log scale\n",
        "plt.title('Cumulative Returns of Selected Assets and SPY (Log Scale)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Return (1 + return, log scale)')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5e0f9ac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate daily returns for SPY\n",
        "spy_returns = daily_returns['SPY']\n",
        "\n",
        "# Calculate Annualized Volatility for SPY (assuming 252 trading days in a year)\n",
        "spy_annualized_volatility = spy_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Calculate Sharpe Ratio for SPY (assuming risk-free rate is 0 for simplicity)\n",
        "spy_sharpe_ratio = spy_returns.mean() / spy_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Calculate Value at Risk (VaR) for SPY\n",
        "# VaR at a certain confidence level (e.g., 95%) represents the maximum expected loss over a given time period.\n",
        "# We'll use the historical method here.\n",
        "confidence_level = 0.95\n",
        "spy_VaR_95 = -np.percentile(spy_returns, 100 * (1 - confidence_level))\n",
        "\n",
        "confidence_level_99 = 0.99\n",
        "spy_VaR_99 = -np.percentile(spy_returns, 100 * (1 - confidence_level_99))\n",
        "\n",
        "\n",
        "# Calculate Conditional Value at Risk (CVaR) for SPY\n",
        "# CVaR (or Expected Shortfall) is the expected loss given that the loss is greater than the VaR.\n",
        "# It provides a more conservative measure of risk than VaR.\n",
        "# We'll calculate CVaR based on the returns that are worse than the VaR.\n",
        "spy_CVaR_95 = -spy_returns[spy_returns < -spy_VaR_95].mean()\n",
        "spy_CVaR_99 = -spy_returns[spy_returns < -spy_VaR_99].mean()\n",
        "\n",
        "\n",
        "display(f\"SPY Annualized Volatility: {spy_annualized_volatility:.4f}\")\n",
        "display(f\"SPY Sharpe Ratio: {spy_sharpe_ratio:.4f}\")\n",
        "display(f\"SPY VaR (95% confidence): {spy_VaR_95:.4f}\")\n",
        "display(f\"SPY CVaR (95% confidence): {spy_CVaR_95:.4f}\")\n",
        "display(f\"SPY VaR (99% confidence): {spy_VaR_99:.4f}\")\n",
        "display(f\"SPY CVaR (99% confidence): {spy_CVaR_99:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44c23746"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate Annualized Volatility (already done, but including for completeness)\n",
        "annualized_volatility = portfolio_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Calculate Sharpe Ratio (already done, but including for completeness)\n",
        "# Assuming risk-free rate is 0 for simplicity\n",
        "sharpe_ratio = portfolio_returns.mean() / portfolio_returns.std() * np.sqrt(252)\n",
        "\n",
        "# Calculate Value at Risk (VaR)\n",
        "# VaR at a certain confidence level (e.g., 95%) represents the maximum expected loss over a given time period.\n",
        "# We'll use the historical method here.\n",
        "confidence_level = 0.95\n",
        "VaR_95 = -np.percentile(portfolio_returns, 100 * (1 - confidence_level))\n",
        "\n",
        "confidence_level_99 = 0.99\n",
        "VaR_99 = -np.percentile(portfolio_returns, 100 * (1 - confidence_level_99))\n",
        "\n",
        "\n",
        "# Calculate Conditional Value at Risk (CVaR)\n",
        "# CVaR (or Expected Shortfall) is the expected loss given that the loss is greater than the VaR.\n",
        "# It provides a more conservative measure of risk than VaR.\n",
        "# We'll calculate CVaR based on the returns that are worse than the VaR.\n",
        "CVaR_95 = -portfolio_returns[portfolio_returns < -VaR_95].mean()\n",
        "CVaR_99 = -portfolio_returns[portfolio_returns < -VaR_99].mean()\n",
        "\n",
        "display(f\"Portfolio Annualized Volatility (Equal Weighted): {annualized_volatility:.4f}\")\n",
        "display(f\"Portfolio Sharpe Ratio (Equal Weighted): {sharpe_ratio:.4f}\")\n",
        "display(f\"Portfolio VaR (95% confidence): {VaR_95:.4f}\")\n",
        "display(f\"Portfolio CVaR (95% confidence): {CVaR_95:.4f}\")\n",
        "display(f\"Portfolio VaR (99% confidence): {VaR_99:.4f}\")\n",
        "display(f\"Portfolio CVaR (99% confidence): {CVaR_99:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1_BiTUXb_N9"
      },
      "outputs": [],
      "source": [
        "# --- Portfolio & Data ---\n",
        "PORTFOLIO_TICKERS = list(filter(lambda item: item != 'SPY', tickers))\n",
        "BENCHMARK_TICKER = 'SPY'\n",
        "DATA_PERIOD = '25y'\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "LOG_RETURN_CLIP = 0.20 # Clip log returns to +/- 20% to remove extreme outliers\n",
        "\n",
        "# --- Bi-LSTM Model ---\n",
        "LOOKBACK_WINDOW = 60 # Days of historical data to use for one prediction\n",
        "TRAIN_SPLIT = 0.64\n",
        "VALIDATION_SPLIT = 0.16 # 16% validation, 20% test\n",
        "LSTM_UNITS = [128, 64, 32]\n",
        "DROPOUT_RATE = 0.2\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- NSGA-II Optimization ---\n",
        "POPULATION_SIZE = 100 # Smaller population for more concentrated solutions\n",
        "N_GENERATIONS = 200   # Fewer generations prevent over-diversification\n",
        "TARGET_DIVIDEND_YIELD = 0.03 # 4% annual dividend yield\n",
        "MIN_WEIGHT_PER_ASSET = 0.01 # 0% min allocation (allow zero weights for assets not selected)\n",
        "MAX_WEIGHT_PER_ASSET = 0.09 # 15% max allocation (allow more concentration in top assets)\n",
        "\n",
        "# --- Prediction Mode ---\n",
        "# 'historical': Use model predictions on the test set for backtesting.\n",
        "# 'future': Generate 30 new forward-looking predictions.\n",
        "PREDICTION_MODE = 'historical'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3d723cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import talib as ta # Import TA-Lib\n",
        "\n",
        "# Define the look-back period\n",
        "look_back = LOOKBACK_WINDOW\n",
        "\n",
        "# Exclude SPY from the close data for portfolio training\n",
        "portfolio_close_data = close_data.drop(columns=['SPY']).copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA SPLIT: Portfolio vs Benchmark\")\n",
        "print(\"=\" * 80)\n",
        "print(f\" Portfolio assets (for Bi-LSTM & NSGA-II): {len(portfolio_close_data.columns)} tickers\")\n",
        "print(f\"  {list(portfolio_close_data.columns)}\")\n",
        "print(f\"\\n Benchmark: SPY (excluded from portfolio)\")\n",
        "print(f\"  SPY in close_data: {'SPY' in close_data.columns}\")\n",
        "print(f\"  SPY in portfolio_close_data: {'SPY' in portfolio_close_data.columns}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# --- Refactor: Build ohlc_data more efficiently ---\n",
        "ohlc_data_parts = []\n",
        "# Track tickers that actually have OHLC data to ensure portfolio_close_data is aligned\n",
        "actual_ohlc_tickers = []\n",
        "\n",
        "for ticker in portfolio_close_data.columns:\n",
        "    # Check for full OHLC data presence (Open, High, Low, Close)\n",
        "    if all(col in data.columns for col in [('Open', ticker), ('High', ticker), ('Low', ticker), ('Close', ticker)]):\n",
        "        temp_df = data[[('Open', ticker), ('High', ticker), ('Low', ticker), ('Close', ticker)]].copy()\n",
        "        temp_df.columns = pd.MultiIndex.from_product([[ticker], ['Open', 'High', 'Low', 'Close']])\n",
        "        ohlc_data_parts.append(temp_df)\n",
        "        actual_ohlc_tickers.append(ticker)\n",
        "    elif ('Close', ticker) in data.columns:\n",
        "        # Fallback to only Close data if full OHLC is not available (e.g., for some ETFs)\n",
        "        temp_df = data[[('Close', ticker)]].copy()\n",
        "        temp_df.columns = pd.MultiIndex.from_product([[ticker], ['Close']])\n",
        "        ohlc_data_parts.append(temp_df)\n",
        "        actual_ohlc_tickers.append(ticker)\n",
        "        print(f\"Only Close data available for {ticker}. Proceeding with Close prices only for feature engineering.\")\n",
        "    else:\n",
        "        # If no relevant data, print a message and this ticker won't be in actual_ohlc_tickers\n",
        "        print(f\"No relevant OHLC data available for {ticker}. Skipping feature engineering for this asset.\")\n",
        "\n",
        "if ohlc_data_parts:\n",
        "    # Concatenate all parts into a single DataFrame\n",
        "    ohlc_data = pd.concat(ohlc_data_parts, axis=1)\n",
        "    # Reindex to match the original portfolio_close_data index and drop NaNs introduced by reindexing/missing data\n",
        "    ohlc_data = ohlc_data.reindex(portfolio_close_data.index).dropna()\n",
        "else:\n",
        "    raise ValueError(\"No OHLC data could be constructed for any asset.\")\n",
        "\n",
        "# Adjust portfolio_close_data to only include assets for which OHLC data was successfully processed and aligned\n",
        "# This ensures consistency for feature calculation base.\n",
        "portfolio_close_data = portfolio_close_data[actual_ohlc_tickers].reindex(ohlc_data.index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eKe8zstb_N-"
      },
      "source": [
        "## Add Enhanced Lag Features and Engineered Variables\n",
        "\n",
        "Adding comprehensive lag features similar to the polars-based approach:\n",
        "- Close price lags (1, 2, 3 days)\n",
        "- High-Minus-Low (HML) and its lags\n",
        "- Open-Minus-Close (OMC) and its lags\n",
        "- Volume lags\n",
        "- Exponential moving averages with different half-lives\n",
        "- Log returns based on lagged prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYdhsREZb_N-"
      },
      "outputs": [],
      "source": [
        "#--- Feature Engineering Function ---\n",
        "def create_features(price_data_polars, tickers):\n",
        "    \"\"\"\n",
        "    Generates a representative set of technical analysis features and targets\n",
        "    for the given tickers, ensuring consistent date ranges and handling missing data robustly.\n",
        "    \"\"\"\n",
        "    print(f\"Generating features for {len(tickers)} tickers...\")\n",
        "\n",
        "    # Get the date range from the already processed Polars DataFrame\n",
        "    start_date = price_data_polars['Date'].min()\n",
        "    end_date = price_data_polars['Date'].max()\n",
        "\n",
        "    print(f\"Downloading OHLCV data from {start_date.date()} to {end_date.date()} for portfolio tickers...\")\n",
        "    ohlcv_data_pd = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "    # Ensure OHLCV data has MultiIndex columns for consistency.\n",
        "    if not isinstance(ohlcv_data_pd.columns, pd.MultiIndex):\n",
        "        raise ValueError(\"Yfinance did not return MultiIndex columns for OHLCV data. Unexpected data structure.\")\n",
        "\n",
        "    # Reindex to align OHLCV data with the dates from the original `price_data_polars`\n",
        "    reference_index = price_data_polars.to_pandas().set_index('Date').index\n",
        "    ohlcv_data_pd = ohlcv_data_pd.reindex(reference_index)\n",
        "\n",
        "    # Fill any NaNs after reindexing to make the data as complete as possible for feature calculations\n",
        "    # First, forward fill, then backward fill to handle leading/trailing NaNs.\n",
        "    # This is applied to the OHLCV data directly.\n",
        "    ohlcv_data_pd = ohlcv_data_pd.ffill().bfill()\n",
        "\n",
        "    # Drop any rows that are still entirely NaN (e.g., if a ticker was completely missing for the entire period)\n",
        "    ohlcv_data_pd = ohlcv_data_pd.dropna(how='all', axis=0)\n",
        "\n",
        "    print(f\"OHLCV data shape after download, reindex, and ffill/bfill: {ohlcv_data_pd.shape}\")\n",
        "\n",
        "    # Identify tickers that have sufficient data (i.e., not entirely NaN in Close price after filling)\n",
        "    clean_tickers = []\n",
        "    filtered_out_tickers = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        if ('Close', ticker) not in ohlcv_data_pd.columns:\n",
        "            print(f\" FILTERED: {ticker} - OHLCV data not found after processing\")\n",
        "            filtered_out_tickers.append((ticker, \"OHLCV data not found\"))\n",
        "            continue\n",
        "        # If Close series is still all NaN after ffill/bfill, it means no data was available for it at all.\n",
        "        if ohlcv_data_pd[('Close', ticker)].isnull().all():\n",
        "            print(f\" FILTERED: {ticker} - No valid Close prices after filling\")\n",
        "            filtered_out_tickers.append((ticker, \"No valid Close prices\"))\n",
        "            continue\n",
        "\n",
        "        # Check data completeness\n",
        "        null_pct = ohlcv_data_pd[('Close', ticker)].isnull().sum() / len(ohlcv_data_pd) * 100\n",
        "        if null_pct > 50:  # If more than 50% missing\n",
        "            print(f\" FILTERED: {ticker} - {null_pct:.1f}% missing data (exceeds 50% threshold)\")\n",
        "            filtered_out_tickers.append((ticker, f\"{null_pct:.1f}% missing data\"))\n",
        "            continue\n",
        "\n",
        "        clean_tickers.append(ticker)\n",
        "\n",
        "    if not clean_tickers:\n",
        "        raise ValueError(\"No valid tickers remaining after filtering for excessive missing data. Cannot create features.\")\n",
        "\n",
        "    print(f\"\\n Proceeding with {len(clean_tickers)} clean tickers out of {len(tickers)} total\")\n",
        "    if filtered_out_tickers:\n",
        "        print(f\"\\n Filtered out {len(filtered_out_tickers)} tickers:\")\n",
        "        for ticker, reason in filtered_out_tickers:\n",
        "            print(f\"   - {ticker}: {reason}\")\n",
        "\n",
        "    feature_dict = {}\n",
        "    for ticker in clean_tickers:\n",
        "        # Extract shifted OHLCV and Volume for feature calculation\n",
        "        # Shift by 1 to prevent look-ahead bias.\n",
        "        high = ohlcv_data_pd[('High', ticker)].shift(1)\n",
        "        low = ohlcv_data_pd[('Low', ticker)].shift(1)\n",
        "        close = ohlcv_data_pd[('Close', ticker)].shift(1)\n",
        "        volume = ohlcv_data_pd[('Volume', ticker)].shift(1)\n",
        "\n",
        "        # --- Momentum Indicators ---\n",
        "        # TA-Lib functions handle leading NaNs by producing NaNs until enough data is available.\n",
        "        feature_dict[f'{ticker}_RSI'] = talib.RSI(close)\n",
        "        macd, macd_signal, macd_hist = talib.MACD(close)\n",
        "        feature_dict[f'{ticker}_MACD'] = macd\n",
        "        feature_dict[f'{ticker}_MACD_SIGNAL'] = macd_signal\n",
        "        feature_dict[f'{ticker}_MACD_HIST'] = macd_hist\n",
        "        feature_dict[f'{ticker}_ADX'] = talib.ADX(high, low, close)\n",
        "        feature_dict[f'{ticker}_ROC'] = talib.ROC(close, timeperiod=LOOKBACK_WINDOW) # New: Rate of Change\n",
        "\n",
        "        # --- Volatility Indicators ---\n",
        "        feature_dict[f'{ticker}_ATR'] = talib.ATR(high, low, close)\n",
        "        upper, middle, lower = talib.BBANDS(close)\n",
        "        feature_dict[f'{ticker}_BB_WIDTH'] = (upper - lower) / middle\n",
        "\n",
        "        # --- Trend Indicators ---\n",
        "        feature_dict[f'{ticker}_SMA_50'] = talib.SMA(close, timeperiod=50)\n",
        "        feature_dict[f'{ticker}_EMA_200'] = talib.EMA(close, timeperiod=200)\n",
        "\n",
        "        # --- Price Lags & Volume ---\n",
        "        feature_dict[f'{ticker}_VOL_SMA_20'] = talib.SMA(volume, timeperiod=20)\n",
        "        for lag in [1, 5, 10, 21]:\n",
        "            feature_dict[f'{ticker}_LAG_{lag}'] = close.pct_change(lag)\n",
        "\n",
        "        # New: Rolling VWAP\n",
        "        # Calculate typical price, then multiply by volume\n",
        "        typical_price = (high + low + close) / 3\n",
        "        typical_price_x_volume = typical_price * volume\n",
        "\n",
        "        # Calculate rolling sums, use min_periods to get values earlier\n",
        "        rolling_typical_price_x_volume_sum = typical_price_x_volume.rolling(window=LOOKBACK_WINDOW, min_periods=1).sum()\n",
        "        rolling_volume_sum = volume.rolling(window=LOOKBACK_WINDOW, min_periods=1).sum()\n",
        "\n",
        "        # Compute VWAP, handling division by zero\n",
        "        vwap = rolling_typical_price_x_volume_sum.div(rolling_volume_sum.replace(0, np.nan))\n",
        "        feature_dict[f'{ticker}_VWAP'] = vwap.replace([np.inf, -np.inf], np.nan) # Replace inf with NaN\n",
        "\n",
        "        # --- Long-Term Features ---\n",
        "        # 1. Long-term Volatility: 252-day rolling standard deviation of returns\n",
        "        returns = close.pct_change()\n",
        "        feature_dict[f'{ticker}_LT_VOL_252'] = returns.rolling(window=252, min_periods=126).std()\n",
        "\n",
        "        # 2. Long-term Position: Price relative to 252-day and 500-day moving averages\n",
        "        sma_252 = talib.SMA(close, timeperiod=252)\n",
        "        sma_500 = talib.SMA(close, timeperiod=500)\n",
        "        feature_dict[f'{ticker}_PRICE_TO_SMA_252'] = close / sma_252\n",
        "        feature_dict[f'{ticker}_PRICE_TO_SMA_500'] = close / sma_500\n",
        "\n",
        "        # 3. Volume Anomaly: 60-day average volume vs. 252-day average volume\n",
        "        vol_sma_60 = talib.SMA(volume, timeperiod=60)\n",
        "        vol_sma_252 = talib.SMA(volume, timeperiod=252)\n",
        "        feature_dict[f'{ticker}_VOL_ANOMALY'] = vol_sma_60 / vol_sma_252\n",
        "\n",
        "    # Create DataFrame efficiently from dictionary. Use the full index from ohlcv_data_pd\n",
        "    features_df = pd.DataFrame(feature_dict, index=ohlcv_data_pd.index)\n",
        "    # Drop columns that are entirely NaN in features_df (e.g., if a feature could not be calculated for a ticker)\n",
        "    features_df = features_df.dropna(axis=1, how='all')\n",
        "    print(f\"features_df shape after creation and dropping all-NaN columns: {features_df.shape}\")\n",
        "\n",
        "    # --- Add VIX as a global feature ---\n",
        "    print(f\"Downloading VIX data (^VIX) for {start_date.date()} to {end_date.date()}...\")\n",
        "    vix_data_pd = yf.download('^VIX', start=start_date, end=end_date, progress=False)\n",
        "    vix_data_pd = vix_data_pd['Close'].reindex(ohlcv_data_pd.index) # Align with the main OHLCV index\n",
        "    vix_data_pd = vix_data_pd.ffill().bfill().shift(1) # Shift to prevent look-ahead bias and fill any NaNs\n",
        "    features_df['VIX_Close'] = vix_data_pd\n",
        "    print(f\"features_df shape after adding VIX: {features_df.shape}\")\n",
        "\n",
        "    # --- Define Target Variable ---\n",
        "    # The target is the next day's log return for each portfolio asset.\n",
        "    # This is calculated on the original (non-shifted) Close price data from ohlcv_data_pd for clean_tickers.\n",
        "    target_df = np.log(ohlcv_data_pd['Close'][clean_tickers] / ohlcv_data_pd['Close'][clean_tickers].shift(1)).shift(-1)\n",
        "    target_df = target_df.clip(-LOG_RETURN_CLIP, LOG_RETURN_CLIP) # Clip outliers\n",
        "    target_df.columns = [f'{col}_TARGET' for col in clean_tickers]\n",
        "    # Drop columns that are entirely NaN in target_df\n",
        "    target_df = target_df.dropna(axis=1, how='all')\n",
        "    print(f\"target_df shape after creation and dropping all-NaN columns: {target_df.shape}\")\n",
        "\n",
        "    # Ensure target_df only contains tickers that still have features after dropping all-NaN feature columns\n",
        "    # Re-align clean_tickers based on available feature columns\n",
        "    final_feature_tickers = sorted(list(set([col.split('_')[0] for col in features_df.columns if not col.endswith('_TARGET') and col != 'VIX_Close'])))\n",
        "    final_target_cols = [f'{t}_TARGET' for t in final_feature_tickers if f'{t}_TARGET' in target_df.columns]\n",
        "    target_df = target_df[final_target_cols] # Filter target_df to match available features\n",
        "    print(f\"target_df shape after aligning with features: {target_df.shape}\")\n",
        "\n",
        "    # --- Combine and clean ---\n",
        "    # Align features and targets on the same index\n",
        "    combined_df = pd.concat([features_df, target_df], axis=1)\n",
        "\n",
        "    # After combining, drop rows that have any NaN, which should now only be due to\n",
        "    # the initial lookback period and the final target shift.\n",
        "    if combined_df.empty:\n",
        "        raise ValueError(\"Combined DataFrame is empty after dropping all-NaN columns. Check data sources or ticker list.\")\n",
        "\n",
        "    combined_df = combined_df.dropna() # Drop rows where any value is NaN\n",
        "\n",
        "    print(f\"\\u2713 Feature engineering complete. Shape: {combined_df.shape}\")\n",
        "    return combined_df, ohlcv_data_pd, clean_tickers\n",
        "\n",
        "price_df = pl.from_pandas(close_data.copy().reset_index().rename(columns={'index': 'Date'}))\n",
        "\n",
        "# --- Execute Feature Engineering ---\n",
        "# We only create features for the assets in our portfolio, not the benchmark\n",
        "features_and_target_df, ohlcv_data_for_cov_target, clean_tickers_for_cov_target = create_features(price_df, PORTFOLIO_TICKERS)\n",
        "\n",
        "# Show which assets were filtered\n",
        "print(f\"\\n Asset Filtering Summary:\")\n",
        "print(f\"   Original portfolio: {len(PORTFOLIO_TICKERS)} assets\")\n",
        "print(f\"   After create_features: {len(clean_tickers_for_cov_target)} assets\")\n",
        "print(f\"   Filtered out: {len(PORTFOLIO_TICKERS) - len(clean_tickers_for_cov_target)} assets\")\n",
        "\n",
        "missing_assets = set(PORTFOLIO_TICKERS) - set(clean_tickers_for_cov_target)\n",
        "if missing_assets:\n",
        "    print(f\"\\n    Missing assets after create_features: {sorted(list(missing_assets))}\")\n",
        "    print(f\"\\n   These assets were removed during feature creation due to:\")\n",
        "    print(f\"      - Insufficient historical data (gaps, missing OHLCV)\")\n",
        "    print(f\"      - Forward/backward fill couldn't reconstruct complete series\")\n",
        "    print(f\"      - Failed technical indicator calculations\")\n",
        "\n",
        "    # Check if the user's 8 assets of interest are in the missing list\n",
        "    user_interest_assets = ['NVDA', 'AMZN', 'AMD', 'INTC', 'META', 'AVGO', 'XOM', 'VDE']\n",
        "    missing_from_interest = [a for a in user_interest_assets if a in missing_assets]\n",
        "    if missing_from_interest:\n",
        "        print(f\"\\n    From your interest list, these {len(missing_from_interest)} were filtered:\")\n",
        "        print(f\"      {missing_from_interest}\")\n",
        "else:\n",
        "    print(f\"\\n    No assets filtered (all {len(PORTFOLIO_TICKERS)} retained)\")\n",
        "\n",
        "print(\"\\nFeatures and Target Data Tail:\")\n",
        "display(features_and_target_df.tail(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKqPgiL8b_N_"
      },
      "outputs": [],
      "source": [
        "def generate_covariance_targets(ohlcv_data_pd, tickers, window_size=LOOKBACK_WINDOW):\n",
        "    \"\"\"\n",
        "    Generates future realized covariance matrices as targets for each date.\n",
        "    For each date, it calculates the covariance of daily log returns over the next `window_size` days.\n",
        "    \"\"\"\n",
        "    print(f\"Generating {window_size}-day future covariance targets for {len(tickers)} tickers...\")\n",
        "\n",
        "    # Calculate log returns for all tickers\n",
        "    log_returns = np.log(ohlcv_data_pd['Close'][tickers] / ohlcv_data_pd['Close'][tickers].shift(1))\n",
        "    log_returns = log_returns.clip(-LOG_RETURN_CLIP, LOG_RETURN_CLIP) # Clip outliers\n",
        "    log_returns = log_returns.dropna() # Drop initial NaNs from shift(1)\n",
        "\n",
        "    covariance_targets = []\n",
        "    target_dates = []\n",
        "\n",
        "    # The features_and_target_df.index represents the date 't' for which we are predicting\n",
        "    # the returns for 't+1' and covariance for 't+1' to 't+1+window_size-1'.\n",
        "    # So we need to look for future returns starting from the day *after* the current_date.\n",
        "    for i in range(len(features_and_target_df.index)):\n",
        "        current_date_for_prediction = features_and_target_df.index[i]\n",
        "\n",
        "        # Find the starting index for the future covariance period in `log_returns`.\n",
        "        # This should be the first entry *after* `current_date_for_prediction`.\n",
        "        start_search_idx = log_returns.index.searchsorted(current_date_for_prediction)\n",
        "\n",
        "        # If searchsorted finds an exact match for current_date_for_prediction, we need to advance one day.\n",
        "        # Otherwise, start_search_idx already points to the first date strictly greater than current_date_for_prediction.\n",
        "        if start_search_idx < len(log_returns) and log_returns.index[start_search_idx] == current_date_for_prediction:\n",
        "            start_cov_period_idx = start_search_idx + 1\n",
        "        else:\n",
        "            start_cov_period_idx = start_search_idx\n",
        "\n",
        "        # Check if there are enough future data points from this starting index\n",
        "        if start_cov_period_idx + window_size > len(log_returns):\n",
        "            covariance_targets.append(np.full(len(tickers) * (len(tickers) + 1) // 2, np.nan)) # Fill with NaNs\n",
        "            target_dates.append(current_date_for_prediction)\n",
        "            continue\n",
        "\n",
        "        future_returns_slice = log_returns.iloc[start_cov_period_idx : start_cov_period_idx + window_size]\n",
        "\n",
        "        # Ensure enough non-NaN data points to calculate covariance within the slice\n",
        "        # If any of the future_returns_slice has NaNs, or if it's not the full window_size, skip\n",
        "        if future_returns_slice.isnull().any().any() or len(future_returns_slice) < window_size:\n",
        "            covariance_targets.append(np.full(len(tickers) * (len(tickers) + 1) // 2, np.nan))\n",
        "        else:\n",
        "            # Calculate covariance matrix\n",
        "            cov_matrix = future_returns_slice.cov()\n",
        "            # Flatten the upper triangular part of the matrix (including diagonal)\n",
        "            # to avoid redundancy and keep target size manageable\n",
        "            # Number of unique elements in a symmetric matrix (n*n) is n*(n+1)/2\n",
        "            flat_cov = cov_matrix.values[np.triu_indices(len(tickers))]\n",
        "            covariance_targets.append(flat_cov)\n",
        "\n",
        "        target_dates.append(current_date_for_prediction)\n",
        "\n",
        "    # Create a DataFrame for the covariance targets\n",
        "    num_cov_elements = len(tickers) * (len(tickers) + 1) // 2\n",
        "    cov_target_df = pd.DataFrame(covariance_targets, index=target_dates,\n",
        "                                 columns=[f'COV_TARGET_{i}' for i in range(num_cov_elements)])\n",
        "\n",
        "    print(f\"\\u2713 Covariance targets generated. Shape: {cov_target_df.shape}\")\n",
        "    return cov_target_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Z16r2Zb_N_"
      },
      "outputs": [],
      "source": [
        "print(\"Regenerating covariance targets and combining with features...\")\n",
        "\n",
        "# 1. Call the `generate_covariance_targets` function\n",
        "covariance_target_df = generate_covariance_targets(ohlcv_data_for_cov_target, clean_tickers_for_cov_target)\n",
        "\n",
        "# 2. Get the clean feature columns (excluding any targets, old or new)\n",
        "feature_cols_only = [c for c in features_and_target_df.columns if not (c.endswith('_TARGET') or c.startswith('COV_TARGET_'))]\n",
        "features_only_df_clean = features_and_target_df[feature_cols_only]\n",
        "\n",
        "# 3. Get the clean return target columns (excluding covariance targets)\n",
        "return_target_cols_clean = [c for c in features_and_target_df.columns if c.endswith('_TARGET') and not c.startswith('COV_TARGET_')]\n",
        "return_targets_df_clean = features_and_target_df[return_target_cols_clean]\n",
        "\n",
        "# Diagnostic: Check how many tickers have return targets\n",
        "print(f\"\\n Return Target Diagnostic:\")\n",
        "print(f\"   Number of return target columns: {len(return_target_cols_clean)}\")\n",
        "print(f\"   Expected from clean_tickers: {len(clean_tickers_for_cov_target)}\")\n",
        "if len(return_target_cols_clean) != len(clean_tickers_for_cov_target):\n",
        "    print(f\"    MISMATCH: {len(clean_tickers_for_cov_target) - len(return_target_cols_clean)} targets missing!\")\n",
        "    expected_targets = [f'{ticker}_TARGET' for ticker in clean_tickers_for_cov_target]\n",
        "    actual_targets = set(return_target_cols_clean)\n",
        "    missing_targets = [t for t in expected_targets if t not in actual_targets]\n",
        "    if missing_targets:\n",
        "        missing_tickers = [t.replace('_TARGET', '') for t in missing_targets]\n",
        "        print(f\"    Missing return targets for: {missing_tickers}\")\n",
        "else:\n",
        "    print(f\"    All {len(return_target_cols_clean)} return targets present\")\n",
        "\n",
        "# 4. Reindex covariance_target_df to match the dates of clean features for consistent merging\n",
        "covariance_target_df = covariance_target_df.reindex(features_only_df_clean.index)\n",
        "\n",
        "# 5. Combine features, original return targets, and the *new* covariance targets cleanly\n",
        "features_and_all_targets_df = pd.concat([\n",
        "    features_only_df_clean,\n",
        "    return_targets_df_clean,\n",
        "    covariance_target_df\n",
        "], axis=1)\n",
        "\n",
        "# 6. Drop any rows with NaN values from `features_and_all_targets_df`\n",
        "# This ensures that each sample has complete feature and target information.\n",
        "rows_before_dropna = len(features_and_all_targets_df)\n",
        "features_and_all_targets_df = features_and_all_targets_df.dropna()\n",
        "rows_after_dropna = len(features_and_all_targets_df)\n",
        "\n",
        "print(f\"\\n NaN Handling Diagnostic:\")\n",
        "print(f\"   Rows before dropna: {rows_before_dropna}\")\n",
        "print(f\"   Rows after dropna: {rows_after_dropna}\")\n",
        "print(f\"   Rows dropped: {rows_before_dropna - rows_after_dropna}\")\n",
        "\n",
        "# Check if any columns (assets) were completely removed\n",
        "columns_before = set(return_target_cols_clean)\n",
        "# After dropna, check which return targets still have any non-null values\n",
        "remaining_return_targets = [c for c in return_target_cols_clean if c in features_and_all_targets_df.columns and features_and_all_targets_df[c].notna().any()]\n",
        "columns_after = set(remaining_return_targets)\n",
        "removed_columns = columns_before - columns_after\n",
        "\n",
        "if removed_columns:\n",
        "    removed_tickers = [c.replace('_TARGET', '') for c in removed_columns]\n",
        "    print(f\"\\n    ASSET REMOVAL DETECTED!\")\n",
        "    print(f\"   {len(removed_tickers)} assets removed after dropna: {removed_tickers}\")\n",
        "\n",
        "    # Check if these match the user's interest list\n",
        "    user_interest_assets = ['NVDA', 'AMZN', 'AMD', 'INTC', 'META', 'AVGO', 'XOM', 'VDE']\n",
        "    removed_from_interest = [t for t in removed_tickers if t in user_interest_assets]\n",
        "    if removed_from_interest:\n",
        "        print(f\"    From your interest list: {removed_from_interest}\")\n",
        "else:\n",
        "    print(f\"    No assets removed (all {len(columns_before)} retained)\")\n",
        "\n",
        "# 7. Update the global variable `features_and_target_df`\n",
        "global features_and_target_df\n",
        "features_and_target_df = features_and_all_targets_df\n",
        "\n",
        "# 8. Redefine `target_cols` based on the newly constructed `features_and_target_df`\n",
        "global target_cols\n",
        "target_cols = [col for col in features_and_target_df.columns if col.endswith('_TARGET') or col.startswith('COV_TARGET_')]\n",
        "\n",
        "# 9. Update the global variable `output_shape`\n",
        "global output_shape\n",
        "output_shape = len(target_cols)\n",
        "\n",
        "# 10. Print the shape of the updated `features_and_target_df` and the count of `target_cols`\n",
        "print(f\"Updated features_and_target_df shape: {features_and_target_df.shape}\")\n",
        "print(f\"Updated target_cols count: {len(target_cols)}\")\n",
        "\n",
        "# 11. Display the tail of the updated `features_and_target_df`\n",
        "print(\"\\nUpdated Features and Target Data Tail:\")\n",
        "display(features_and_target_df.tail(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKHbApnMb_N_"
      },
      "outputs": [],
      "source": [
        "print(\"Preparing data for LSTM model...\")\n",
        "\n",
        "# Separate feature columns from target columns\n",
        "feature_cols = [c for c in features_and_target_df.columns if not (c.endswith('_TARGET') or c.startswith('COV_TARGET_'))]\n",
        "\n",
        "# Identify return target columns and covariance target columns\n",
        "return_target_cols = [c for c in features_and_target_df.columns if c.endswith('_TARGET') and not c.startswith('COV_TARGET_')]\n",
        "cov_target_cols = [c for c in features_and_target_df.columns if c.startswith('COV_TARGET_')]\n",
        "\n",
        "# Determine the number of targets\n",
        "num_return_targets = len(return_target_cols)\n",
        "num_cov_targets = len(cov_target_cols)\n",
        "print(f\"Number of return targets: {num_return_targets}\")\n",
        "print(f\"Number of covariance targets: {num_cov_targets}\")\n",
        "\n",
        "# Extract features and targets\n",
        "X = features_and_target_df[feature_cols].values\n",
        "y_returns = features_and_target_df[return_target_cols].values\n",
        "y_cov = features_and_target_df[cov_target_cols].values\n",
        "\n",
        "# Scale features and targets separately\n",
        "feature_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "returns_scaler = MinMaxScaler(feature_range=(-1, 1)) # Scaler for return targets\n",
        "cov_scaler = MinMaxScaler(feature_range=(-1, 1))     # Scaler for covariance targets\n",
        "\n",
        "X_scaled = feature_scaler.fit_transform(X)\n",
        "y_returns_scaled = returns_scaler.fit_transform(y_returns)\n",
        "y_cov_scaled = cov_scaler.fit_transform(y_cov)\n",
        "\n",
        "# --- 2. Create Sequences ---\n",
        "def create_sequences(X_data, y_returns_data, y_cov_data, window_size):\n",
        "    X_seq, y_returns_seq, y_cov_seq = [], [], []\n",
        "    for i in range(len(X_data) - window_size):\n",
        "        X_seq.append(X_data[i:(i + window_size)])\n",
        "        y_returns_seq.append(y_returns_data[i + window_size])\n",
        "        y_cov_seq.append(y_cov_data[i + window_size])\n",
        "    return np.array(X_seq), np.array(y_returns_seq), np.array(y_cov_seq)\n",
        "\n",
        "X_seq, y_returns_seq, y_cov_seq = create_sequences(X_scaled, y_returns_scaled, y_cov_scaled, LOOKBACK_WINDOW)\n",
        "\n",
        "# --- 3. Train/Val/Test Split (Temporal) ---\n",
        "n_samples = len(X_seq)\n",
        "train_end = int(n_samples * TRAIN_SPLIT)\n",
        "val_end = int(n_samples * (TRAIN_SPLIT + VALIDATION_SPLIT))\n",
        "\n",
        "X_train, y_train_returns, y_train_cov = X_seq[:train_end], y_returns_seq[:train_end], y_cov_seq[:train_end]\n",
        "X_val, y_val_returns, y_val_cov = X_seq[train_end:val_end], y_returns_seq[train_end:val_end], y_cov_seq[train_end:val_end]\n",
        "X_test, y_test_returns, y_test_cov = X_seq[val_end:], y_returns_seq[val_end:], y_cov_seq[val_end:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train_returns shape: {y_train_returns.shape}, y_train_cov shape: {y_train_cov.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val_returns shape: {y_val_returns.shape}, y_val_cov shape: {y_val_cov.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test_returns shape: {y_test_returns.shape}, y_test_cov shape: {y_test_cov.shape}\")\n",
        "\n",
        "# --- 4. Model Architecture & Training (Modified) ---\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "def create_advanced_bilstm_model(input_shape, num_return_targets, num_cov_targets, lstm_units, dropout_rate):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(lstm_units[0], return_sequences=True))(inputs)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Bidirectional(LSTM(lstm_units[1], return_sequences=True))(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    attention_output = Attention()(x)\n",
        "    x = Dropout(dropout_rate)(attention_output)\n",
        "\n",
        "    # Output for returns\n",
        "    returns_output = Dense(num_return_targets, activation='tanh', name='returns_output')(x)\n",
        "\n",
        "    # Output for covariance\n",
        "    covariance_output = Dense(num_cov_targets, activation='linear', name='covariance_output')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[returns_output, covariance_output])\n",
        "    model.compile(optimizer='adam', loss={'returns_output': Huber(), 'covariance_output': 'mean_squared_error'})\n",
        "    return model\n",
        "\n",
        "model = create_advanced_bilstm_model(input_shape, num_return_targets, num_cov_targets, LSTM_UNITS, DROPOUT_RATE)\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('bilstm_best_model.keras', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    {'returns_output': y_train_returns, 'covariance_output': y_train_cov},\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, {'returns_output': y_val_returns, 'covariance_output': y_val_cov}),\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"--- Model Training Complete ---\")\n",
        "\n",
        "# --- 5. Plot Training History ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['returns_output_loss'], label='Training Returns Loss')\n",
        "plt.plot(history.history['val_returns_output_loss'], label='Validation Returns Loss')\n",
        "plt.plot(history.history['covariance_output_loss'], label='Training Covariance Loss')\n",
        "plt.plot(history.history['val_covariance_output_loss'], label='Validation Covariance Loss')\n",
        "plt.title('Model Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e91d1888"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained Bi-LSTM model to make predictions on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8j2je7Lb_N_"
      },
      "source": [
        "### Classification Analysis: Direction Prediction Accuracy\n",
        "\n",
        "Convert continuous return predictions to binary up/down signals and evaluate classification performance with confusion matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlmvYeSEb_N_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions on test set - model has multiple outputs\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Extract returns predictions (first output)\n",
        "if isinstance(predictions, list):\n",
        "    y_pred_returns_scaled = predictions[0]  # Returns are the first output\n",
        "else:\n",
        "    y_pred_returns_scaled = predictions\n",
        "\n",
        "# Inverse transform to get actual return values\n",
        "y_pred_returns = returns_scaler.inverse_transform(y_pred_returns_scaled)\n",
        "y_actual_returns = returns_scaler.inverse_transform(y_test_returns)\n",
        "\n",
        "# Get the correct number of assets\n",
        "num_assets = y_pred_returns.shape[1]\n",
        "asset_tickers = portfolio_close_data.columns[:num_assets]\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "y_pred_df = pd.DataFrame(y_pred_returns, columns=asset_tickers)\n",
        "y_actual_df = pd.DataFrame(y_actual_returns, columns=asset_tickers)\n",
        "\n",
        "print(f\"Predictions shape: {y_pred_returns.shape}\")\n",
        "print(f\"Actual shape: {y_actual_returns.shape}\")\n",
        "print(f\"Number of assets: {num_assets}\")\n",
        "print(f\"Asset tickers: {list(asset_tickers)}\")\n",
        "\n",
        "# Convert continuous returns to binary signals (1 = Up/Positive, 0 = Down/Negative)\n",
        "y_pred_signals = (y_pred_df > 0).astype(int)\n",
        "y_actual_signals = (y_actual_df > 0).astype(int)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DIRECTIONAL PREDICTION ACCURACY - CLASSIFICATION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nConverting continuous returns to binary up/down signals:\")\n",
        "print(\"   Signal = 1 (UP): Predicted/Actual return > 0\")\n",
        "print(\"   Signal = 0 (DOWN): Predicted/Actual return  0\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Calculate metrics for each asset\n",
        "accuracy_scores = {}\n",
        "for ticker in asset_tickers:\n",
        "    accuracy = accuracy_score(y_actual_signals[ticker], y_pred_signals[ticker])\n",
        "    accuracy_scores[ticker] = accuracy\n",
        "\n",
        "# Overall accuracy across all predictions\n",
        "overall_accuracy = accuracy_score(y_actual_signals.values.flatten(),\n",
        "                                  y_pred_signals.values.flatten())\n",
        "\n",
        "print(f\"\\n{'Asset':<10} {'Directional Accuracy':>20}\")\n",
        "print(\"-\" * 35)\n",
        "for ticker, acc in sorted(accuracy_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{ticker:<10} {acc:>19.2%}\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"{'OVERALL':<10} {overall_accuracy:>19.2%}\")\n",
        "\n",
        "# Create confusion matrices for selected assets (top 6 by accuracy)\n",
        "top_assets = sorted(accuracy_scores.items(), key=lambda x: x[1], reverse=True)[:6]\n",
        "top_asset_tickers = [t[0] for t in top_assets]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Confusion Matrices: Direction Prediction (UP vs DOWN)',\n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "\n",
        "for idx, ticker in enumerate(top_asset_tickers):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_actual_signals[ticker], y_pred_signals[ticker])\n",
        "\n",
        "    # Plot heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['DOWN (0)', 'UP (1)'],\n",
        "                yticklabels=['DOWN (0)', 'UP (1)'],\n",
        "                cbar=True, ax=ax, annot_kws={'size': 14})\n",
        "\n",
        "    ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{ticker} - Accuracy: {accuracy_scores[ticker]:.2%}',\n",
        "                fontsize=12, fontweight='bold', pad=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report for overall performance\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OVERALL CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nAggregated across all assets and time steps:\\n\")\n",
        "print(classification_report(y_actual_signals.values.flatten(),\n",
        "                          y_pred_signals.values.flatten(),\n",
        "                          target_names=['DOWN (0)', 'UP (1)'],\n",
        "                          digits=4))\n",
        "\n",
        "# Overall confusion matrix\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OVERALL CONFUSION MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "cm_overall = confusion_matrix(y_actual_signals.values.flatten(),\n",
        "                              y_pred_signals.values.flatten())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_overall, annot=True, fmt='d', cmap='YlGnBu',\n",
        "            xticklabels=['DOWN (0)', 'UP (1)'],\n",
        "            yticklabels=['DOWN (0)', 'UP (1)'],\n",
        "            cbar_kws={'label': 'Count'},\n",
        "            annot_kws={'size': 16, 'fontweight': 'bold'})\n",
        "plt.xlabel('Predicted Direction', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Actual Direction', fontsize=13, fontweight='bold')\n",
        "plt.title(f'Overall Confusion Matrix\\nAccuracy: {overall_accuracy:.2%}',\n",
        "         fontsize=14, fontweight='bold', pad=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate precision for UP predictions (important for trading)\n",
        "tn, fp, fn, tp = cm_overall.ravel()\n",
        "precision_up = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall_up = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_up = 2 * (precision_up * recall_up) / (precision_up + recall_up) if (precision_up + recall_up) > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY METRICS FOR UP PREDICTIONS (Trading Signals)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Precision (UP): {precision_up:.2%}    When model predicts UP, it's correct {precision_up:.2%} of time\")\n",
        "print(f\"Recall (UP):    {recall_up:.2%}    Model captures {recall_up:.2%} of actual UP movements\")\n",
        "print(f\"F1-Score (UP):  {f1_up:.2%}    Harmonic mean of precision and recall\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDXe97hIb_OA"
      },
      "outputs": [],
      "source": [
        "print(\"Generating predictions from the trained Bi-LSTM model...\")\n",
        "# Load the best model saved during training\n",
        "best_model = tf.keras.models.load_model('bilstm_best_model.keras', custom_objects={'Huber': Huber, 'Attention': Attention})\n",
        "\n",
        "# Predict on the test set, which now returns two outputs\n",
        "predicted_returns_scaled, predicted_covariances_scaled = best_model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions for returns and covariances\n",
        "predicted_log_returns = returns_scaler.inverse_transform(predicted_returns_scaled)\n",
        "predicted_covariances_flattened = cov_scaler.inverse_transform(predicted_covariances_scaled)\n",
        "\n",
        "# Convert log returns to simple returns\n",
        "predicted_simple_returns = np.expm1(predicted_log_returns)\n",
        "\n",
        "# Create DataFrame for predicted simple returns\n",
        "predicted_returns_df = pd.DataFrame(predicted_simple_returns, columns=return_target_cols)\n",
        "\n",
        "print(f\" Predicted returns generated. Shape: {predicted_returns_df.shape}\")\n",
        "print(f\" Predicted flattened covariances generated. Shape: {predicted_covariances_flattened.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QNd76jub_OA"
      },
      "outputs": [],
      "source": [
        "def flattened_to_cov_matrix(flattened_cov, n_assets):\n",
        "    \"\"\"\n",
        "    Reconstructs a full symmetric covariance matrix from its flattened upper triangular form.\n",
        "    \"\"\"\n",
        "    cov_matrix = np.zeros((n_assets, n_assets))\n",
        "    # Get indices for the upper triangle (including diagonal)\n",
        "    triu_indices = np.triu_indices(n_assets)\n",
        "    # Fill the upper triangle\n",
        "    cov_matrix[triu_indices] = flattened_cov\n",
        "    # Fill the lower triangle to make it symmetric\n",
        "    cov_matrix = cov_matrix + cov_matrix.T - np.diag(np.diag(cov_matrix))\n",
        "    return cov_matrix\n",
        "\n",
        "# Apply the helper function to each predicted flattened covariance\n",
        "n_assets = len(PORTFOLIO_TICKERS)\n",
        "predicted_cov_matrices_time_series = [\n",
        "    flattened_to_cov_matrix(row, n_assets) for row in predicted_covariances_flattened\n",
        "]\n",
        "\n",
        "print(f\" Generated {len(predicted_cov_matrices_time_series)} predicted covariance matrices.\")\n",
        "print(f\"Example of a predicted covariance matrix shape: {predicted_cov_matrices_time_series[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c55e2b49"
      },
      "source": [
        "## Define the optimization problem\n",
        "\n",
        "### Subtask:\n",
        "Clearly define the objectives (e.g., maximize predicted return, maximize predicted dividend yield, minimize predicted risk - volatility/CVaR) and constraints (e.g., sum of weights equals 1, minimum/maximum allocation per asset) for the portfolio optimization using NSGA-II.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "555a189d"
      },
      "source": [
        "**Reasoning**:\n",
        "Document the objectives and constraints for the NSGA-II algorithm based on the task description and the referenced paper's likely approach to multi-objective portfolio optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b61f706"
      },
      "outputs": [],
      "source": [
        "# Objectives for NSGA-II:\n",
        "# 1. Maximize Predicted Portfolio Return: Based on the price predictions from the Bi-LSTM model.\n",
        "# 2. Maximize Predicted Portfolio Dividend Yield: Based on the predicted dividends and predicted prices.\n",
        "# 3. Minimize Predicted Portfolio Risk: This can be measured using predicted portfolio volatility or Conditional Value at Risk (CVaR) calculated from the predicted returns.\n",
        "\n",
        "# Constraints for NSGA-II:\n",
        "# 1. Sum of Weights: The sum of the weights of all assets in the portfolio must equal 1. ( wi = 1)\n",
        "# 2. Weight Bounds: Define minimum and maximum allocation percentages for each asset (e.g., 0 <= wi <= 1, or with more specific bounds like 0.05 <= wi <= 0.4).\n",
        "# 3. Target Dividend Yield (Implicit): The NSGA-II will aim to find solutions on the Pareto front that satisfy the 4% dividend return requirement. This might be handled as a soft constraint or by selecting solutions from the Pareto front that meet this criterion.\n",
        "\n",
        "print(\"NSGA-II Objectives:\")\n",
        "print(\"1. Maximize Predicted Portfolio Return\")\n",
        "print(\"2. Maximize Predicted Portfolio Dividend Yield\")\n",
        "print(\"3. Minimize Predicted Portfolio Risk (Volatility/CVaR)\")\n",
        "print(\"\\nNSGA-II Constraints:\")\n",
        "print(\"1. Sum of Weights = 1\")\n",
        "print(\"2. Weight Bounds (e.g., 0 <= wi <= 1)\")\n",
        "print(\"3. Target Dividend Yield (Implicit objective for solution selection)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d27e88f"
      },
      "source": [
        "## Prepare input data for nsga-ii\n",
        "\n",
        "### Subtask:\n",
        "Prepare the predicted stock movements and dividend information in a format suitable for the NSGA-II algorithm. This might involve using the predictions from the Bi-LSTM model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LDH5Oatb_OA"
      },
      "source": [
        "##  Simplified Pipeline: Bi-LSTM  NSGA-II\n",
        "\n",
        "**Original Complex Approach:**\n",
        "1. Model predicts log returns \n",
        "2. Convert log returns  prices\n",
        "3. Compare with previous actual prices\n",
        "4. Calculate returns from price differences\n",
        "5. Run Monte Carlo simulation (unused)\n",
        "6. Pass returns to NSGA-II\n",
        "\n",
        "**New Simplified Approach:**\n",
        "1. Model predicts log returns \n",
        "2. Convert to simple returns: `r = exp(log_r) - 1` \n",
        "3. Pass directly to NSGA-II \n",
        "\n",
        "**Benefits:**\n",
        "-  Fewer transformation steps = less error accumulation\n",
        "-  No dependency on previous actual prices (avoids look-ahead issues)\n",
        "-  Faster execution (no unnecessary Monte Carlo)\n",
        "-  Direct connection: model output  optimizer input\n",
        "-  Cleaner code and easier to debug\n",
        "\n",
        "**What NSGA-II receives:**\n",
        "- DataFrame of predicted daily returns (simple returns)\n",
        "- Shape: (n_test_days, n_assets)\n",
        "- Used to calculate portfolio return, volatility, and Sharpe ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99133406"
      },
      "source": [
        "## Prepare Predicted Returns for NSGA-II\n",
        "\n",
        "**Simplified Approach:**\n",
        "Since the Bi-LSTM model now predicts log returns (not prices), we can directly use those predictions for NSGA-II optimization. This eliminates the need to:\n",
        "- Convert predictions back to prices\n",
        "- Compare with previous actual prices\n",
        "- Recalculate returns\n",
        "\n",
        "**Process:**\n",
        "1. Take predicted log returns from model\n",
        "2. Convert to simple returns: `r = exp(log_r) - 1`\n",
        "3. Pass directly to NSGA-II\n",
        "\n",
        "This is more direct, avoids compounding errors, and maintains consistency with the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37305JHDb_OA"
      },
      "source": [
        "##  Configure NSGA-II Input: Choose Prediction Mode\n",
        "\n",
        "You can now choose which predictions to use for portfolio optimization:\n",
        "\n",
        "**Option 1: Historical Test Set Predictions** (already available)\n",
        "- Uses the test set period (e.g., 2023-03 to 2025-09)\n",
        "- Large sample size (~600+ days)\n",
        "- Can be validated against actual returns\n",
        "- Good for backtesting and model evaluation\n",
        "\n",
        "**Option 2: 30-Day Forward Predictions** (new capability)\n",
        "- Predicts 30 days into the future from today\n",
        "- Smaller sample (30 days)\n",
        "- Forward-looking portfolio optimization\n",
        "- Best for real-world trading decisions\n",
        "\n",
        "**Which to use?**\n",
        "- For **backtesting and validation**: Use historical test predictions\n",
        "- For **actual portfolio selection**: Use 30-day forward predictions\n",
        "- For **research**: Compare both approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a66ba0e2"
      },
      "source": [
        "## Run the NSGA-II Optimization\n",
        "\n",
        "### Subtask:\n",
        "Execute the NSGA-II algorithm to find a set of non-dominated portfolios (the Pareto front)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea81a0a"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate the portfolio optimization problem and run the NSGA-II algorithm from pymoo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "931330a8"
      },
      "outputs": [],
      "source": [
        "print(\"Updating PortfolioOptimizationProblem class...\")\n",
        "\n",
        "# Extract test_dates corresponding to the X_test data points\n",
        "# X_seq, y_seq are created from features_and_target_df starting from index 0\n",
        "# X_test starts from index `val_end` of X_seq\n",
        "# So, the dates for X_test start from `features_and_target_df.index[val_end + LOOKBACK_WINDOW]`\n",
        "# and for each sample in X_test, the corresponding date is its index.\n",
        "\n",
        "# The original features_and_target_df has one entry for each date, and X_seq was built from it.\n",
        "# The first `LOOKBACK_WINDOW` rows were effectively used to create the first sequence, so the first\n",
        "# prediction corresponds to the date at index `LOOKBACK_WINDOW`.\n",
        "# X_test starts at index `val_end` of X_seq. The date for the first sample in X_test is at\n",
        "# `features_and_target_df.index[val_end + LOOKBACK_WINDOW]`.\n",
        "\n",
        "test_dates = features_and_target_df.index[val_end + LOOKBACK_WINDOW : val_end + LOOKBACK_WINDOW + len(X_test)]\n",
        "\n",
        "class PortfolioOptimizationProblem(Problem):\n",
        "    def __init__(self, mu, dividend_yields, predicted_returns_timeseries,\n",
        "                 predicted_cov_matrices_time_series, test_dates,\n",
        "                 min_weight, max_weight, target_dividend, max_portfolio_volatility=None, min_portfolio_return=None):\n",
        "\n",
        "        # Add 4th constraint if min_portfolio_return is specified\n",
        "        n_constraints = 2\n",
        "        if max_portfolio_volatility is not None:\n",
        "            n_constraints += 1\n",
        "        if min_portfolio_return is not None:\n",
        "            n_constraints += 1\n",
        "\n",
        "        super().__init__(n_var=len(mu),\n",
        "                         n_obj=4,\n",
        "                         n_constr=n_constraints, # sum of weights = 1, dividend yield >= target, [optional] volatility <= max, [optional] return >= min\n",
        "                         xl=min_weight,\n",
        "                         xu=max_weight)\n",
        "        self.mu = mu\n",
        "        self.dividend_yields = dividend_yields\n",
        "        self.predicted_returns_timeseries = predicted_returns_timeseries\n",
        "        self.predicted_cov_matrices_time_series = predicted_cov_matrices_time_series\n",
        "        self.test_dates = test_dates # Index for mapping predicted covariance matrices\n",
        "        self.target_dividend = target_dividend\n",
        "        self.max_portfolio_volatility = max_portfolio_volatility\n",
        "        self.min_portfolio_return = min_portfolio_return\n",
        "\n",
        "    def _evaluate(self, x, out, *args, **kwargs):\n",
        "        # Normalize weights to ensure they sum to 1\n",
        "        weights = x / x.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # Initialize objectives arrays\n",
        "        f1 = np.zeros(x.shape[0]) # Negative Return\n",
        "        f2 = np.zeros(x.shape[0]) # Negative Dividend Yield\n",
        "        f3 = np.zeros(x.shape[0]) # Volatility\n",
        "        f4 = np.zeros(x.shape[0]) # CVaR\n",
        "\n",
        "        # Initialize constraints arrays\n",
        "        g1 = np.zeros(x.shape[0]) # Sum of weights\n",
        "        g2 = np.zeros(x.shape[0]) # Dividend yield constraint\n",
        "        constraint_idx = 3\n",
        "        if self.max_portfolio_volatility is not None:\n",
        "            g3 = np.zeros(x.shape[0]) # Maximum volatility constraint\n",
        "            constraint_idx += 1\n",
        "        if self.min_portfolio_return is not None:\n",
        "            g_return = np.zeros(x.shape[0]) # Minimum return constraint\n",
        "\n",
        "        for i, current_weights in enumerate(weights):\n",
        "            # Use the predicted covariance matrix that corresponds to the current test period\n",
        "            # The `mu` and `predicted_cov_matrices_time_series` are aligned with the test set.\n",
        "            # The `predicted_cov_matrices_time_series` is already a list indexed 0 to len(X_test)-1.\n",
        "            # So we can directly use `i` as the index.\n",
        "            current_sigma = self.predicted_cov_matrices_time_series[i % len(self.predicted_cov_matrices_time_series)]\n",
        "\n",
        "            # 1. Maximize Annualized Return (minimize negative return)\n",
        "            annualized_return = np.dot(current_weights, self.mu) * 252\n",
        "            f1[i] = -annualized_return\n",
        "\n",
        "            # 2. Maximize Dividend Yield (minimize negative yield)\n",
        "            f2[i] = -np.sum(current_weights * self.dividend_yields)\n",
        "\n",
        "            # 3. Minimize Annualized Volatility\n",
        "            portfolio_volatility = np.sqrt(np.dot(current_weights.T, np.dot(current_sigma, current_weights)))\n",
        "            annualized_volatility = portfolio_volatility * np.sqrt(252)\n",
        "            f3[i] = annualized_volatility\n",
        "\n",
        "            # 4. Minimize CVaR (95%)\n",
        "            # CVaR is calculated using the predicted returns timeseries for the portfolio\n",
        "            ts_idx = i % len(self.predicted_returns_timeseries)\n",
        "            portfolio_returns_ts = (self.predicted_returns_timeseries[ts_idx] @ current_weights)\n",
        "            q = np.percentile(portfolio_returns_ts, 5)\n",
        "            cvar = np.mean(portfolio_returns_ts[portfolio_returns_ts <= q])\n",
        "            f4[i] = -cvar * 252 # Minimize CVaR (less negative is better)\n",
        "\n",
        "            # --- Constraints ---\n",
        "            # Constraint 1: sum of weights must be 1 (pymoo handles this as g <= 0)\n",
        "            # Since we normalize weights above, this should always be ~0, but we keep small tolerance\n",
        "            g1[i] = np.abs(np.sum(current_weights) - 1.0) - 1e-6  # Allow small numerical error\n",
        "\n",
        "            # Constraint 2: portfolio dividend yield must be >= target\n",
        "            # g <= 0 means: target - actual <= 0, or actual >= target\n",
        "            portfolio_yield = np.sum(current_weights * self.dividend_yields)\n",
        "            g2[i] = self.target_dividend - portfolio_yield\n",
        "\n",
        "            # Constraint 3: portfolio volatility must be <= max_portfolio_volatility\n",
        "            # g <= 0 means: actual - max <= 0, or actual <= max\n",
        "            if self.max_portfolio_volatility is not None:\n",
        "                g3[i] = annualized_volatility - self.max_portfolio_volatility\n",
        "\n",
        "            # Constraint 4: portfolio return must be >= min_portfolio_return\n",
        "            # g <= 0 means: min - actual <= 0, or actual >= min\n",
        "            if self.min_portfolio_return is not None:\n",
        "                g_return[i] = self.min_portfolio_return - annualized_return\n",
        "\n",
        "        out[\"F\"] = np.column_stack([f1, f2, f3, f4])\n",
        "\n",
        "        # Build constraints dynamically\n",
        "        constraints = [g1, g2]\n",
        "        if self.max_portfolio_volatility is not None:\n",
        "            constraints.append(g3)\n",
        "        if self.min_portfolio_return is not None:\n",
        "            constraints.append(g_return)\n",
        "        out[\"G\"] = np.column_stack(constraints)\n",
        "\n",
        "print(\" PortfolioOptimizationProblem class updated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjyYGMnMb_OB"
      },
      "outputs": [],
      "source": [
        "# --- Run NSGA-II Optimization ---\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.optimize import minimize\n",
        "\n",
        "print(\"Running NSGA-II optimization...\")\n",
        "\n",
        "# Create the optimization problem with constraints\n",
        "problem = PortfolioOptimizationProblem(\n",
        "    mu=mu,\n",
        "    dividend_yields=avg_annual_yield,\n",
        "    predicted_returns_timeseries=predicted_simple_returns,\n",
        "    predicted_cov_matrices_time_series=predicted_cov_matrices_time_series,\n",
        "    test_dates=test_dates,\n",
        "    min_weight=MIN_WEIGHT_PER_ASSET,\n",
        "    max_weight=MAX_WEIGHT_PER_ASSET,\n",
        "    target_dividend=TARGET_DIVIDEND_YIELD,\n",
        "    max_portfolio_volatility=0.22,  # 22% max portfolio volatility\n",
        "    min_portfolio_return=0.09  # 9% minimum return\n",
        ")\n",
        "\n",
        "# Configure NSGA-II algorithm\n",
        "algorithm = NSGA2(\n",
        "    pop_size=POPULATION_SIZE,\n",
        "    eliminate_duplicates=True\n",
        ")\n",
        "\n",
        "# Run optimization\n",
        "res = minimize(\n",
        "    problem,\n",
        "    algorithm,\n",
        "    ('n_gen', N_GENERATIONS),\n",
        "    seed=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- Process Results ---\n",
        "print(f\"\\n Optimization complete!\")\n",
        "print(f\"   Found {len(res.F)} solutions on the Pareto front\")\n",
        "\n",
        "# Extract solutions\n",
        "if hasattr(res, 'F') and res.F is not None:\n",
        "    # Get solutions from Pareto front\n",
        "    solutions = res.X\n",
        "    objectives = res.F\n",
        "    print(f\"   Using Pareto front: {len(solutions)} solutions\")\n",
        "else:\n",
        "    # Fallback to population\n",
        "    solutions = res.pop.get(\"X\")\n",
        "    objectives = res.pop.get(\"F\")\n",
        "    print(f\"   Using population: {len(solutions)} solutions\")\n",
        "\n",
        "# Normalize weights (ensure they sum to 1)\n",
        "weight_sums = solutions.sum(axis=1, keepdims=True)\n",
        "weight_sums = np.where(weight_sums == 0, 1, weight_sums)  # Avoid division by zero\n",
        "solutions_normalized = solutions / weight_sums\n",
        "\n",
        "# Create results DataFrame\n",
        "pareto_results = pd.DataFrame(solutions_normalized, columns=asset_tickers)\n",
        "pareto_results['Return'] = -objectives[:, 0]  # Negate because we minimized\n",
        "pareto_results['Dividend'] = -objectives[:, 1]  # Negate because we minimized\n",
        "pareto_results['Volatility'] = objectives[:, 2]\n",
        "pareto_results['CVaR'] = objectives[:, 3]\n",
        "pareto_results['Sharpe'] = (pareto_results['Return'] - 0.02) / pareto_results['Volatility']\n",
        "\n",
        "print(\"\\nPareto Front Summary:\")\n",
        "print(f\"Return range: [{pareto_results['Return'].min():.4f}, {pareto_results['Return'].max():.4f}]\")\n",
        "print(f\"Dividend range: [{pareto_results['Dividend'].min():.4f}, {pareto_results['Dividend'].max():.4f}]\")\n",
        "print(f\"Volatility range: [{pareto_results['Volatility'].min():.4f}, {pareto_results['Volatility'].max():.4f}]\")\n",
        "print(f\"CVaR range: [{pareto_results['CVaR'].min():.4f}, {pareto_results['CVaR'].max():.4f}]\")\n",
        "print(f\"Sharpe range: [{pareto_results['Sharpe'].min():.4f}, {pareto_results['Sharpe'].max():.4f}]\")\n",
        "\n",
        "# --- Visualize Pareto Front ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Return vs Volatility (colored by Sharpe)\n",
        "scatter1 = axes[0].scatter(\n",
        "    pareto_results['Volatility']*100,\n",
        "    pareto_results['Return']*100,\n",
        "    c=pareto_results['Sharpe'],\n",
        "    cmap='viridis',\n",
        "    s=100,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black'\n",
        ")\n",
        "# Highlight highest Sharpe\n",
        "best_sharpe_idx = pareto_results['Sharpe'].idxmax()\n",
        "axes[0].scatter(\n",
        "    pareto_results.loc[best_sharpe_idx, 'Volatility']*100,\n",
        "    pareto_results.loc[best_sharpe_idx, 'Return']*100,\n",
        "    c='red',\n",
        "    s=300,\n",
        "    marker='*',\n",
        "    edgecolors='black',\n",
        "    linewidths=2,\n",
        "    label='Highest Sharpe',\n",
        "    zorder=5\n",
        ")\n",
        "axes[0].set_xlabel('Annualized Volatility (%)', fontsize=12)\n",
        "axes[0].set_ylabel('Expected Annual Return (%)', fontsize=12)\n",
        "axes[0].set_title('Efficient Frontier: Return vs Volatility', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Sharpe Ratio')\n",
        "\n",
        "# Plot 2: Return vs Dividend (colored by Volatility)\n",
        "scatter2 = axes[1].scatter(\n",
        "    pareto_results['Dividend']*100,\n",
        "    pareto_results['Return']*100,\n",
        "    c=pareto_results['Volatility']*100,\n",
        "    cmap='coolwarm',\n",
        "    s=100,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black'\n",
        ")\n",
        "# Highlight highest Sharpe\n",
        "axes[1].scatter(\n",
        "    pareto_results.loc[best_sharpe_idx, 'Dividend']*100,\n",
        "    pareto_results.loc[best_sharpe_idx, 'Return']*100,\n",
        "    c='red',\n",
        "    s=300,\n",
        "    marker='*',\n",
        "    edgecolors='black',\n",
        "    linewidths=2,\n",
        "    label='Highest Sharpe',\n",
        "    zorder=5\n",
        ")\n",
        "axes[1].set_xlabel('Dividend Yield (%)', fontsize=12)\n",
        "axes[1].set_ylabel('Expected Annual Return (%)', fontsize=12)\n",
        "axes[1].set_title('Return vs Dividend Yield', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Volatility (%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Visualization complete!\")\n",
        "print(f\"   Best Sharpe Ratio: {pareto_results.loc[best_sharpe_idx, 'Sharpe']:.4f}\")\n",
        "print(f\"   Return: {pareto_results.loc[best_sharpe_idx, 'Return']:.4%}\")\n",
        "print(f\"   Volatility: {pareto_results.loc[best_sharpe_idx, 'Volatility']:.4%}\")\n",
        "print(f\"   Dividend: {pareto_results.loc[best_sharpe_idx, 'Dividend']:.4%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-VaLuEBb_OC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Select the Best Portfolio for Backtesting ---\n",
        "print(\"Selecting portfolio with the highest Sharpe Ratio for backtesting...\")\n",
        "best_sharpe_idx = pareto_results['Sharpe'].idxmax()\n",
        "selected_portfolio = pareto_results.loc[best_sharpe_idx]\n",
        "selected_weights = selected_portfolio[asset_tickers]\n",
        "\n",
        "print(\"\\nSelected Portfolio Weights (allocations > .1%):\")\n",
        "display(selected_weights[selected_weights > 0.001].sort_values(ascending=False))\n",
        "\n",
        "# --- 2. Calculate Historical Performance ---\n",
        "# Use the ACTUAL returns from the test period, not the predicted returns\n",
        "test_dates_start = features_and_target_df.index[val_end + LOOKBACK_WINDOW]\n",
        "\n",
        "# Removed redundant calculations. Assuming 'price_df_pd' is the correct DataFrame.\n",
        "actual_returns_test = price_df_pd.loc[test_dates_start:, asset_tickers].pct_change().dropna()\n",
        "benchmark_returns_test = price_df_pd.loc[test_dates_start:, BENCHMARK_TICKER].pct_change().dropna()\n",
        "\n",
        "# Align indices before matrix multiplication\n",
        "aligned_weights = selected_weights.values\n",
        "aligned_returns, aligned_benchmark = actual_returns_test.align(benchmark_returns_test, join='inner', axis=0)\n",
        "\n",
        "portfolio_historical_returns = (aligned_returns * aligned_weights).sum(axis=1)\n",
        "\n",
        "# --- 3. Calculate Performance Metrics ---\n",
        "def calculate_metrics(returns, risk_free_rate=0.02):\n",
        "    \"\"\"Calculates key performance metrics for a series of returns.\"\"\"\n",
        "    if returns.empty:\n",
        "        return pd.Series({k: 'N/A' for k in ['Cumulative Return', 'Annualized Return', 'Annualized Volatility', 'Sharpe Ratio', 'Max Drawdown']})\n",
        "\n",
        "    cumulative_return = (1 + returns).prod() - 1\n",
        "    annualized_return = returns.mean() * 252\n",
        "    annualized_volatility = returns.std() * np.sqrt(252)\n",
        "    sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility if annualized_volatility != 0 else 0\n",
        "\n",
        "    # Max Drawdown Calculation\n",
        "    cumulative_wealth = (1 + returns).cumprod()\n",
        "    peak = cumulative_wealth.cummax()\n",
        "    drawdown = (cumulative_wealth - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Cumulative Return': f\"{cumulative_return:.2%}\",\n",
        "        'Annualized Return': f\"{annualized_return:.2%}\",\n",
        "        'Annualized Volatility': f\"{annualized_volatility:.2%}\",\n",
        "        'Sharpe Ratio': f\"{sharpe_ratio:.2f}\",\n",
        "        'Max Drawdown': f\"{max_drawdown:.2%}\"\n",
        "    })\n",
        "\n",
        "# Define the risk-free rate as a constant for clarity\n",
        "RISK_FREE_RATE = 0.02\n",
        "portfolio_metrics = calculate_metrics(portfolio_historical_returns, risk_free_rate=RISK_FREE_RATE)\n",
        "benchmark_metrics = calculate_metrics(aligned_benchmark, risk_free_rate=RISK_FREE_RATE)\n",
        "\n",
        "performance_summary = pd.DataFrame({'Optimized Portfolio': portfolio_metrics, 'Benchmark (SPY)': benchmark_metrics})\n",
        "\n",
        "print(\"\\nBacktest Performance Summary:\")\n",
        "display(performance_summary)\n",
        "\n",
        "# --- 5. Visualize Performance ---\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Use the pre-aligned returns series for consistency and robustness\n",
        "cumulative_portfolio_returns = (1 + portfolio_historical_returns).cumprod()\n",
        "cumulative_benchmark_returns = (1 + aligned_benchmark).cumprod()\n",
        "\n",
        "plt.plot(cumulative_portfolio_returns.index, cumulative_portfolio_returns, label='Optimized Portfolio', lw=2)\n",
        "plt.plot(cumulative_benchmark_returns.index, cumulative_benchmark_returns, label='Benchmark (SPY)', linestyle='--', lw=2)\n",
        "\n",
        "plt.title('Backtest: Optimized Portfolio vs. Benchmark (SPY)', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Cumulative Growth of $1', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DriJQn3ob_OC"
      },
      "outputs": [],
      "source": [
        "# export the weights to a csv file\n",
        "selected_weights.to_csv('highest_sharpe_portfolio_weights.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}